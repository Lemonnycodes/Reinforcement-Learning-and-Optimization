# Reinforcement-Learning-and-Optimization

This repository contains implementations of Reinforcement learning and optimization algorithms focused on improving decision-making in dynamic environments. The project showcases a blend of traditional optimization techniques and modern Reinforcement Leaning approaches to solve complex problems in pathfinding, Decision-making, and policy optimization.

## Project Structure

- **Bug Algorithms**: Implementations of Bug 0, Bug 1, and Bug 2 algorithms for pathfinding and obstacle avoidance.
- **Constraint-Based Optimization**: Solutions to Mixed-Integer Linear Programming (MILP) problems with enhancements via obstacle incorporation.
- **MDPs and Bandits**: Code for Markov Decision Processes (MDPs) and bandit algorithms like Epsilon-Greedy and UCB.
- **Q-Learning and MCTS**: Applications of Q-Learning and Monte Carlo Tree Search (MCTS) to discover optimal policies.
- **Policy Gradient Methods**: Implementations of REINFORCE and Actor-Critic methods for policy optimization in complex environments.

## Technologies

- **Programming Language**: Python
- **Libraries**: NumPy, pandas, Matplotlib

## Features

### Bug Algorithms
- **Efficiency Improvement**: Achieved a 21.5% improvement in path length efficiency with Bug 2 compared to Bug 0.

### Constraint-Based Optimization
- **Objective Value Achievement**: Reached an objective value of 780.0.
- **Optimization Improvement**: 90.7% improvement in objective value through dynamic obstacle adjustment.

### MDPs and Bandits
- **Fast Convergence**: Value iteration achieved convergence within 19.70 seconds.
- **Optimal Decision-Making**: Epsilon-Greedy and UCB algorithms achieved optimal pull percentages of 89.76% and 90.87%.

### Q-Learning and MCTS
- **Rapid Policy Convergence**: Q-Learning algorithm converged to an optimal policy within 10,000 episodes.
- **Efficient Policy Discovery**: MCTS discovered optimal policies within 1000 iterations.

### Policy Gradient Methods
- **Reward Improvements**: REINFORCE and Actor-Critic methods improved cumulative rewards by 15% and 18%.
